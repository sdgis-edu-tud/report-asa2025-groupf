# Typology Construction

```{r}
#| echo: false
#| output: false
library(sf) ## for processing vector data 
library(dplyr) ## for selecting and transforming data
library(plotly) ## for plotting data

source("../src/attributes.R")

## View the columns of the data
colnames(spatial_units)

used_attributes_cols = c(
  "Green_Area",
  "Space_along_Streams",
  "Shade_3pm",
  "Accessibility"
)

## Build used_attributes from used_attributes_cols and attribute_to_col
used_attributes <- setNames(
  names(attribute_to_col)[match(used_attributes_cols, unlist(attribute_to_col))],
  used_attributes_cols
)

n_attr <- length(used_attributes)
```

The objective that we wanted to address with Typology Construction was ...
To do so, we used the following attributes that were initially computed for the MCDA analysis:

- `r paste(used_attributes, collapse = "\n- ")`

Below are what these layers look like:

```{r}
#| echo: false
#| layout-ncol: 2

## Plot the attributes in two columns
for (col in names(used_attributes)) {
     plot(spatial_units[col], border = NA, main = used_attributes[[col]])
}
```

Then, we normalised each feature to have an average of 0 and a standard deviation of 1.

```{r}
#| echo: false
#| label: fig-elbow
#| fig-cap: Visualisation of the inertia for different numbers of clusters ofr the elbow method.

features <- spatial_units |> 
  select(names(used_attributes)) |>
  st_drop_geometry() ## remove geometry column so we just keep a data table

X_scaled <- scale(features) ## Standardize (mean=0, sd=1) 

## Initialize an empty numeric vector to store inertia values 
inertia <- numeric()

## Try k values from 2 to 9
k_values <- 2:9

## Loop through each k value
for (k in k_values) {
  km <- kmeans(X_scaled, centers = k, iter.max=25, nstart = 50) 
  
  ## tot.withinss is Total within-cluster sum of squares
  ## This measures how compact the clusters are: lower is better.
  inertia <- c(inertia, km$tot.withinss)
}

## Combine the results into a data frame for plotting
elbow_df <- data.frame(k = k_values, inertia = inertia)

## Make the elbow plot
plot(k_values, inertia,
     type = "b",
     col = "darkblue",
     main = "Elbow Method")

## Choose the number of clusters based on the elbow plot
n_clusters <- 4
```

To use k-means properly to cluster the data, we had to select a good number of clusters.
To do so, we used the **elbow method**.
We tried values of $k$ from 2 to 9, and looking at the graph of the inertia for all values in @fig-elbow, we picked $k=`r n_clusters`$ at the value with the largest elbow angle.

Then, we ran the k-means algorithm and to assign a final cluster to each spatial unit.

```{r}
#| echo: false
#| label: tbl-n_per_cluster
#| tbl-cap: Spatial units per cluster

## `set.seed()` sets the random number generator to a fixed state
## Set the seed so the clustering result is always the same when re-run
set.seed(0)  ## The number 0 is just a fixed choice. You can also use 10, 345, etc.

# Run K-means clustering on the standardized data
kmeans_result <- kmeans(X_scaled, centers = n_clusters, nstart = 20)

## Add the cluster labels to the spatial data
spatial_units$Cluster <- as.factor(kmeans_result$cluster) ## The result kmeans_result$cluster is a list of cluster labels (1 to 4), in the same order as the original rows in X_scaled and spatial_units

## Show how many spatial_units fall into each cluster
print(table(spatial_units$Cluster))
```

We can see with @tbl-n_per_cluster that each cluster contains a decent number of points.
Then, we plotted the clusters geographically in the spatial units in @fig-clusters-units.

```{r}
#| echo: false
#| label: fig-clusters-units
#| fig-cap: Visualisation of the spatial units clustered geographically

## Generate cluster colors dynamically
cluster_levels <- levels(spatial_units$Cluster)
cluster_colors_all <- palette.colors(palette = "Okabe-Ito")
cluster_colors <- setNames(cluster_colors_all[seq_len(n_clusters)], cluster_levels)

## Plot clusters with base R
plot(spatial_units["Cluster"], 
     main = "Spatial Pattern of Urban Stream Clusters", 
     border = NA,
     col = cluster_colors[spatial_units$Cluster])

legend("topright",
     legend = levels(spatial_units$Cluster),
     fill = cluster_colors,
     title = "Cluster",
     cex = 0.9,
     bty = "n")
```

```{r}
## Get the cluster centers (in standardized form)
scaled_centers <- kmeans_result$centers

## Convert the centers back to original scale: x * SD + mean
original_centers <- t(apply(
  scaled_centers, 1, 
  function(x) x * attr(X_scaled, "scaled:scale") + attr(X_scaled, "scaled:center")
))

## Print the real-world values
print(original_centers, digits=3)

```

To interpret the clustering results, we examine the centers of each cluster in the original data scale.
Then, we try to assign a typology to each of the clusters based on these values in @tbl-clusters-interpretation.

| Type | Green-blue connectivity | Available space near streams | Shade | Peripherality | Description |
|----|--------|----------|--------|--------|--------------------|
| 1 | 0.952 | 0.0235 | 0.539 | 0.923 | ........... |
| 2 | 0.259 | 0.1689 | 0.147 | 0.836 | ........... |
| 3 | 0.156 | 0.0846 | 0.318 | 0.401 | ........... |
| 4 | 0.692 | 0.6540 | 0.121 | 0.814 | ........... |

: Interpretation of the clusters {#tbl-clusters-interpretation}

```{r}
#| echo: false

### Compute distance to cluster center and store the output

## Get the cluster center for each row, using the cluster assignment
centroids_matrix <- kmeans_result$centers[kmeans_result$cluster, ]

## Calculate Euclidean distance between each point and its assigned cluster center
spatial_units_distances <- sqrt(rowSums((X_scaled - centroids_matrix)^2))

## Save to spatial_units
spatial_units$Cluster_Dist <- spatial_units_distances
st_write(spatial_units, "../data/results/Spatial_Units-Typology_Construction.geojson", delete_dsn = TRUE, quiet = TRUE)
```

Another interesting point about this clustering is that all the variables we used have very different distributions, as can be seen from @fig-columns-distribution.
The fact that some of them have a high concentration of values on the extreme values makes it quite likely to have a cluster to be centred on this value for this attribute, and this is indeed what we can see in the output.

```{r}
#| echo: false
#| label: fig-columns-distribution
#| fig-cap: Distribution of the variables

## calculate nornalization
normalize <- function(x) (x - min(x)) / (max(x) - min(x))
features_norm <- as.data.frame(lapply(features, normalize))

## Set up color palette for any number of features (distinct colors)
set.seed(42) # for reproducibility
hist_colors <- grDevices::hcl.colors(n_attr, palette = "Dynamic", alpha = 1.0)

## Set canvas: 2 columns for original and standardized
par(mfrow = c(n_attr, 2), mar = c(4, 4, 2, 1))

## Plot histograms for each attribute
for (i in seq_along(used_attributes)) {
  colname <- names(used_attributes)[i]
  display <- used_attributes[[colname]]
  color <- hist_colors[i]

  # Original
  hist(features[[colname]],
       main = paste0(display, "\n(original)"),
       xlab = "value", col = color)

  # Standardized
  hist(X_scaled[, colname],
       main = paste0(display, "\n(standardized)"),
       xlab = "z-score", col = color)
}
```
